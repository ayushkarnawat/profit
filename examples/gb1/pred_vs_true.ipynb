{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:48:27.588113Z",
     "start_time": "2020-06-02T17:48:20.120294Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pgf import FigureCanvasPgf\n",
    "\n",
    "mpl.backend_bases.register_backend(\"pdf\", FigureCanvasPgf)\n",
    "mpl.use(\"pgf\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format=\"retina\"\n",
    "\n",
    "# Matplotlib plotting options\n",
    "plt.style.use(\"seaborn-paper\")\n",
    "plt.rcParams.update({\n",
    "    \"axes.labelsize\": 22,               # label size (x- and y-axis)\n",
    "    \"axes.titlesize\": 26,               # title size on (most) axes\n",
    "    \"figure.titlesize\": 26,             # title size for fig.sup_title()\n",
    "    \"legend.fontsize\": \"x-large\",       # font size for legend\n",
    "    \"lines.markersize\": 6,              # marker size for points and lines\n",
    "    \"lines.markeredgewidth\": 2,         # marker edgewidth for points\n",
    "    \"xtick.labelsize\": 18,              # label size for x-ticks \n",
    "    \"ytick.labelsize\": 18,              # label size for y-ticks\n",
    "\n",
    "    \"font.family\": \"serif\",             # use serif/main font for text elements\n",
    "    \"text.usetex\": True,                # use inline math for ticks\n",
    "    \"pgf.rcfonts\": False,               # don't setup fonts from rc params\n",
    "    \"pgf.preamble\": [\n",
    "        # Syling\n",
    "        r\"\\usepackage{color}\",          # special colors\n",
    "        r\"\\setmainfont{DejaVu Serif}\",  # serif font via preamble\n",
    "        r\"\\usepackage{bm}\",             # bold text\n",
    "\n",
    "        # Math\n",
    "        r\"\\usepackage{xfrac}\",          # side fractions\n",
    "        r\"\\usepackage{amsthm}\",         # theorems\n",
    "        r\"\\usepackage{amsmath}\",        # misc math\n",
    "        r\"\\usepackage{amssymb}\",        # blackboard math symbols\n",
    "        r\"\\usepackage{mathtools}\",      # enhance the appearance of math\n",
    "    ],\n",
    "})\n",
    "\n",
    "from profit.models.torch import SequenceOracle, SequenceGPR\n",
    "from profit.utils.data_utils.tokenizers import AminoAcidTokenizer\n",
    "from profit.utils.data_utils.serializers import LMDBSerializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:48:27.741396Z",
     "start_time": "2020-06-02T17:48:27.720306Z"
    }
   },
   "outputs": [],
   "source": [
    "def bbox_to_rect(bbox, color):\n",
    "    # Convert to matplotlib format: ((upper-left x, upper-left y), width, height).\n",
    "    return plt.Rectangle(xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0],\n",
    "                             height=bbox[3]-bbox[1], fill=False,\n",
    "                             edgecolor=color, linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:48:27.905578Z",
     "start_time": "2020-06-02T17:48:27.855044Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess + load the dataset\n",
    "dataset = LMDBSerializer.load(\"../../data/3gb1/processed/lstm_fitness/primary_encoding=aa20.mdb\")\n",
    "Xaa = dataset[:][\"arr_0\"].long()\n",
    "_labels = dataset[:][\"arr_1\"].view(-1)\n",
    "\n",
    "# Determine vocab and sequence length\n",
    "tokenizer = AminoAcidTokenizer(\"aa20\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "seqlen = Xaa.size(1)\n",
    "\n",
    "# Convert to onehot\n",
    "X = torch.zeros(*Xaa.size(), vocab_size)\n",
    "X.scatter_(2, torch.unsqueeze(Xaa, 2), 1)\n",
    "X.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:48:28.057026Z",
     "start_time": "2020-06-02T17:48:28.029024Z"
    }
   },
   "outputs": [],
   "source": [
    "paths = sorted(glob.glob(\"../../bin/3gb1/oracle/*/E*\"))\n",
    "metadata = [\"all\", \"all-weighted\", \"g-mean\", \"l-mean\", \"g-median\", \"l-median\", \"g-zero\"]\n",
    "oracle_paths = dict(zip(metadata, paths))\n",
    "oracle_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:48:28.326949Z",
     "start_time": "2020-06-02T17:48:28.201086Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize and load weights for all oracles\n",
    "oracle_stump = SequenceOracle(seqlen, vocab_size, hidden_size=50, out_size=2)\n",
    "all_oracles = {}\n",
    "for desc, path in oracle_paths.items():\n",
    "    oracle = copy.deepcopy(oracle_stump)\n",
    "    oracle.load_state_dict(torch.load(path))\n",
    "    all_oracles[desc] = oracle\n",
    "\n",
    "# Evaluate on oracle to get predictions\n",
    "preds = {}\n",
    "for desc, oracle in all_oracles.items():\n",
    "    oracle.eval()\n",
    "    with torch.no_grad():\n",
    "        oracle_pred = oracle(X)[:,0]\n",
    "        preds[desc] = oracle_pred\n",
    "        \n",
    "# Print topk oracle (pred) values \n",
    "for desc, pred in preds.items():\n",
    "    idx = torch.sort(pred).indices[-10:]\n",
    "    print(desc.upper(), pred[idx], _labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:48:31.781925Z",
     "start_time": "2020-06-02T17:48:29.029990Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16,6))\n",
    "xy = np.linspace(_labels.min(), _labels.max()) # x = y\n",
    "colors = cm.get_cmap(\"coolwarm\")(np.linspace(0, 1, len(preds)))\n",
    "\n",
    "# First graph\n",
    "handles = []\n",
    "for (desc, pred), c in zip(preds.items(), colors):\n",
    "    h = ax1.scatter(_labels, pred, color=c, edgecolors=\"black\", label=desc)\n",
    "    handles.append(h)\n",
    "ax1.plot(xy, xy, \"--\", c=\"black\")\n",
    "ax1.tick_params(\"both\")\n",
    "# ax1.set_xlabel(\"Ground Truth $y$\")\n",
    "ax1.set_ylabel(\"Mean Oracle Prediction $\\hat{y}$\")\n",
    "ax1.legend(handles, list(preds.keys()), scatterpoints=1)\n",
    "\n",
    "# Add bounding box (view of 2nd graph)\n",
    "bbox = np.array([-0.1, 2.75, 2, -0.2])\n",
    "ax1.add_patch(bbox_to_rect(bbox, 'black'))\n",
    "\n",
    "# Second graph\n",
    "for (desc, pred), c in zip(preds.items(), colors):\n",
    "    ax2.scatter(_labels, pred, color=c, edgecolors=\"black\", label=desc)\n",
    "ax2.plot(xy, xy, \"--\", c=\"black\")\n",
    "ax2.tick_params(\"both\")\n",
    "# ax2.set_xlabel(\"Ground Truth $y$\")\n",
    "ax2.legend(handles, list(preds.keys()), scatterpoints=1)\n",
    "ax2.set_xlim(bbox[[0,2]])\n",
    "ax2.set_ylim(bbox[[1,-1]][::-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "# fig.suptitle(\"Oracle comparison\")\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see is that oracle predictions are very poor, when compared to the true GT. In fact, for most sequences, the oracle predicts approximately the same values (with some small variance). This is true even when we train the oracle on a subset of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:48:32.770199Z",
     "start_time": "2020-06-02T17:48:32.759468Z"
    }
   },
   "outputs": [],
   "source": [
    "paths = sorted(glob.glob(\"../../bin/3gb1/gpr/*\"))\n",
    "metadata = [\"all\", \"g-mean\", \"l-mean\", \"g-median\", \"l-median\", \"g-zero\"]\n",
    "gpr_paths = dict(zip(metadata, paths))\n",
    "gpr_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:48:48.703697Z",
     "start_time": "2020-06-02T17:48:33.542612Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize and load weights for all gprs\n",
    "all_gps = {desc: torch.load(path) for desc, path in gpr_paths.items()}\n",
    "\n",
    "# Evaluate on oracle to get predictions\n",
    "preds = {}\n",
    "for desc, gp in all_gps.items():\n",
    "    y_mean = gp.predict(Xaa, return_std=False)\n",
    "    preds[desc] = y_mean\n",
    "\n",
    "# Print topk oracle (pred) values\n",
    "for desc, pred in preds.items():\n",
    "    pred = pred.squeeze() # flatten array\n",
    "    idx = torch.sort(pred).indices[-10:]\n",
    "    print(desc.upper(), pred[idx], _labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:48:52.252925Z",
     "start_time": "2020-06-02T17:48:49.759230Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16,6))\n",
    "xy = np.linspace(_labels.min(), _labels.max()) # x = y\n",
    "colors = cm.get_cmap(\"coolwarm\")(np.linspace(0, 1, len(preds)))\n",
    "\n",
    "# First graph\n",
    "handles = []\n",
    "for (desc, pred), c in zip(preds.items(), colors):\n",
    "    h = ax1.scatter(_labels, pred, color=c, edgecolors=\"black\", label=desc)\n",
    "    handles.append(h)\n",
    "ax1.plot(xy, xy, \"--\", c=\"black\")\n",
    "ax1.tick_params(\"both\")\n",
    "ax1.set_xlabel(\"Ground Truth $y$\")\n",
    "ax1.set_ylabel(\"Mean Oracle Prediction $\\hat{y}$\")\n",
    "ax1.legend(handles, list(preds.keys()), scatterpoints=1)\n",
    "\n",
    "# Add bounding box (zoomed in view of 2nd graph)\n",
    "bbox = np.array([-0.02, 0.25, 0.25, -0.2])\n",
    "ax1.add_patch(bbox_to_rect(bbox, 'black'))\n",
    "\n",
    "# Second graph\n",
    "for (desc, pred), c in zip(preds.items(), colors):\n",
    "    ax2.scatter(_labels, pred, color=c, edgecolors=\"black\", label=desc)\n",
    "ax2.plot(xy, xy, \"--\", c=\"black\")\n",
    "ax2.tick_params(\"both\")\n",
    "ax2.set_xlabel(\"Ground Truth $y$\")\n",
    "ax2.legend(handles, list(preds.keys()), scatterpoints=1)\n",
    "ax2.set_xlim(bbox[[0,2]])\n",
    "ax2.set_ylim(bbox[[1,-1]][::-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "# fig.suptitle(\"Training data\")\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see is that predictions are good, but often a little lower than true GT. This might be because a lot of sequences in the original dataset are 0. Even if we train the kernel on the sequences that are greater than 0, the performance is still sub-par at best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condition by Adaptive Sampling (CbAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select optimal oracle/gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to select the oracle and GT in such a fashion that (a) the oracle values are somewhat improving with each iteration and (b) the GT is not falling substantially (aka being led astray into \"poor\" regions of the search space) with each iteration. Although not strictly required, it is also important to select the oracle/GT such that the initial training dataset used to train both is the same.\n",
    "\n",
    "What do we do if the oracle is particularly bad at determining the fitness values between good and bad sequences? This is what is currently happening (see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:48:53.421810Z",
     "start_time": "2020-06-02T17:48:53.396315Z"
    }
   },
   "outputs": [],
   "source": [
    "cbas_files = sorted(glob.glob(\"../../dumps/3gb1/cbas/*.json\"))\n",
    "cbas_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:48:58.083482Z",
     "start_time": "2020-06-02T17:48:55.059918Z"
    }
   },
   "outputs": [],
   "source": [
    "colors = cm.get_cmap(\"coolwarm\")(np.linspace(0, 1, len(cbas_files)))\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "for file, c in zip(cbas_files, colors):\n",
    "    with open(file, \"rb\") as json_file:\n",
    "        values = json.load(json_file)\n",
    "        # Compute mean and std of the samples for each experiment\n",
    "        oracle_mean = np.mean(np.ma.masked_invalid(values[\"y_oracle\"]), axis=-1)\n",
    "        oracle_std = np.std(np.ma.masked_invalid(values[\"y_oracle\"]), axis=-1)\n",
    "        gt_mean = np.mean(np.ma.masked_invalid(values[\"y_gt\"]), axis=-1)\n",
    "        gt_std = np.mean(np.ma.masked_invalid(values[\"y_gt\"]), axis=-1)\n",
    "        # Plot results (error bars for last iteration)\n",
    "        plt.scatter(oracle_mean, gt_mean, color=c, edgecolors=\"black\")\n",
    "        plt.errorbar(oracle_mean[-1], gt_mean[-1], xerr=oracle_std[-1], yerr=oracle_std[-1],\n",
    "                     color=c, fmt='none')\n",
    "\n",
    "plt.xlabel(r\"$y_{\\mathrm{oracle(s)}}$\")\n",
    "plt.ylabel(r\"$y_{\\mathrm{gt}}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, we see that as number of iterations increase, both the GT and the oracle predictions (for the samples in that sequence) increases until the oracle reaches its asymptotic upper limit. This means that we have likely converge onto a decent sequence candidate set of sequences (within each iteration) which are not contained in the original dataset. Note that the GT values is largely ignored (still useful to determine if the model is not being led into \"bad\" sequence regions of the search space).\n",
    "\n",
    "We want to plot the trajectory of the top sequence (for both the oracle and the GT) across each of the 42 different runs. This will help us determine which is the \"best\" subset of the dataset to train the model(s) on. For instance, if the GT does not decrease substantially while the oracle is predicting high values that means the dataset is doing good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:49:09.545077Z",
     "start_time": "2020-06-02T17:48:59.467687Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "quantile = 0.80\n",
    "sort = True\n",
    "\n",
    "for oracle_key in oracle_paths.keys():\n",
    "    # Gather values (for all GPs) for specified oracle\n",
    "    plt.figure(figsize=(12,8))\n",
    "    filepaths = [file for file in cbas_files if f\"oracle={oracle_key}__\" in file]\n",
    "    gp_keys = [os.path.split(file)[-1].split(\".\")[0].split(\"__\")[-1] for file in filepaths]\n",
    "    colors = cm.get_cmap(\"coolwarm\")(np.linspace(0, 1, len(filepaths)))\n",
    "    for file, gp_key, c in zip(filepaths, gp_keys, colors):\n",
    "        with open(file, \"rb\") as json_file:\n",
    "            values = json.load(json_file)\n",
    "        # Compute 80th percentile range for y_oracle\n",
    "        oracle_samples = np.ma.masked_invalid(values[\"y_oracle\"])\n",
    "        per = np.percentile(oracle_samples, quantile*100, axis=-1)\n",
    "        per = np.reshape(per, newshape=(oracle_samples.shape[0], 1))\n",
    "        oracle_idxs = np.where(oracle_samples > per)\n",
    "        \n",
    "        # Average values for samples that are greater than quantile\n",
    "        oracle_vals = np.zeros_like(oracle_samples)\n",
    "        oracle_vals[oracle_idxs] = oracle_samples[oracle_idxs]\n",
    "        oracle_vals = oracle_vals.sum(1) / (oracle_vals != 0).sum(1)\n",
    "        \n",
    "        gt_samples = np.ma.masked_invalid(values[\"y_gt\"])\n",
    "        gt_vals = np.zeros_like(gt_samples)\n",
    "        gt_vals[oracle_idxs] = gt_samples[oracle_idxs]\n",
    "        gt_vals = gt_vals.sum(1) / (gt_vals != 0).sum(1)\n",
    "        \n",
    "        # Sort oracle values (ascending) when plotting\n",
    "        if sort:\n",
    "            oracle_idxs = np.argsort(oracle_vals)\n",
    "            oracle_vals = oracle_vals[oracle_idxs]\n",
    "            gt_vals = gt_vals[oracle_idxs]\n",
    "        \n",
    "        # Plot values\n",
    "        plt.plot(np.arange(1, len(oracle_vals)+1), oracle_vals, color=c, linestyle=\"dashed\")\n",
    "        plt.plot(np.arange(1, len(gt_vals)+1), gt_vals, color=c, label=gp_key)\n",
    "    plt.xlabel(\"Timestep ($t$)\")\n",
    "    plt.ylabel(f\"${quantile * 100:.0f}\\%$ Fitness ($y$)\")\n",
    "    plt.title(f\"oracle={oracle_key}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topk (k=10) sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above trajectories, it seems that no matter the oracle used, the GP trained on indices greater than the mean, seems to perform well in that it does not explore \"poor\" regions of the sequence-space (as evidence by the fact that the GP remains around its max value). As for the oracle, the best seems to be g-mean/g-median/g-zero as well (since they produce high GT values). Should we compare the topk sequences from each of these runs to see if they concur on similar sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:28:52.473082Z",
     "start_time": "2020-06-02T19:28:52.410815Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../../dumps/3gb1/cbas/oracle=g-mean__gp=g-mean.json\", \"rb\") as json_file:\n",
    "    cbas_results = json.load(json_file)\n",
    "\n",
    "# What are the topk sequences and their predicted oracle/GT values\n",
    "oracles = np.array(cbas_results[\"y_oracle\"]).flatten()\n",
    "topk_idx = np.argsort(oracles)[-10:]\n",
    "\n",
    "seq_topk = np.array(cbas_results[\"seq\"]).flatten()[topk_idx]\n",
    "yt_topk = oracles[topk_idx]\n",
    "yt_gt_topk = np.array(cbas_results[\"y_gt\"]).flatten()[topk_idx]\n",
    "\n",
    "print(yt_topk, np.mean(yt_topk))\n",
    "print(yt_gt_topk, np.mean(yt_gt_topk))\n",
    "print(seq_topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T18:38:11.477396Z",
     "start_time": "2020-06-02T18:38:11.418902Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../../dumps/3gb1/cbas/oracle=all-weighted__gp=all.json\", \"rb\") as json_file:\n",
    "    cbas_results = json.load(json_file)\n",
    "\n",
    "# What are the topk sequences and their predicted oracle/GT values\n",
    "oracles = np.array(cbas_results[\"y_oracle\"]).flatten()\n",
    "topk_idx = np.argsort(oracles)[-10:]\n",
    "\n",
    "seq_topk = np.array(cbas_results[\"seq\"]).flatten()[topk_idx]\n",
    "yt_topk = oracles[topk_idx]\n",
    "yt_gt_topk = np.array(cbas_results[\"y_gt\"]).flatten()[topk_idx]\n",
    "\n",
    "print(yt_topk)\n",
    "print(yt_gt_topk)\n",
    "print(seq_topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T18:38:14.226272Z",
     "start_time": "2020-06-02T18:38:14.173455Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../../dumps/3gb1/cbas/oracle=g-mean__gp=all.json\", \"rb\") as json_file:\n",
    "    cbas_results = json.load(json_file)\n",
    "\n",
    "# What are the topk sequences and their predicted oracle/GT values\n",
    "oracles = np.array(cbas_results[\"y_oracle\"]).flatten()\n",
    "topk_idx = np.argsort(oracles)[-10:]\n",
    "\n",
    "seq_topk = np.array(cbas_results[\"seq\"]).flatten()[topk_idx]\n",
    "yt_topk = oracles[topk_idx]\n",
    "yt_gt_topk = np.array(cbas_results[\"y_gt\"]).flatten()[topk_idx]\n",
    "\n",
    "print(yt_topk)\n",
    "print(yt_gt_topk)\n",
    "print(seq_topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:28:56.539326Z",
     "start_time": "2020-06-02T19:28:56.527817Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from profit.dataset import generator\n",
    "from profit.dataset.preprocessors import LSTMPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:28:59.717019Z",
     "start_time": "2020-06-02T19:28:59.701244Z"
    }
   },
   "outputs": [],
   "source": [
    "positions = np.arange(20, 56)\n",
    "num_mutation_sites = len(positions)\n",
    "seqs = generator.gen(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:29:02.670102Z",
     "start_time": "2020-06-02T19:29:02.558777Z"
    }
   },
   "outputs": [],
   "source": [
    "# Permute each position with amino acid\n",
    "preprocessor = LSTMPreprocessor(vocab=\"aa20\")\n",
    "all_seqs = []\n",
    "for pos in positions:\n",
    "    for seq in seqs:\n",
    "        template = list(\"MTYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTE\")\n",
    "        template[pos] = seq\n",
    "        all_seqs.append(preprocessor.get_input_feats(template))\n",
    "all_seqs = torch.Tensor(all_seqs).long()\n",
    "\n",
    "# Convert to onehot\n",
    "S = torch.zeros(*all_seqs.size(), vocab_size)\n",
    "S.scatter_(2, torch.unsqueeze(all_seqs, dim=2), 1)\n",
    "\n",
    "# Evaluate on (g-mean) oracle to get predictions\n",
    "gmean_oracle = all_oracles[\"g-zero\"]\n",
    "if isinstance(gmean_oracle, SequenceGPR):\n",
    "    mu_oracle, sigma_oracle = gmean_oracle.predict(all_seqs, return_std=True)\n",
    "    mu_oracle, sigma_oracle = mean_oracle.squeeze(), sigma_oracle.squeeze()\n",
    "else:\n",
    "    gmean_oracle.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = gmean_oracle(S)\n",
    "        mu_oracle, sigma_oracle = y_pred[:,0], torch.sqrt(y_pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:29:06.209077Z",
     "start_time": "2020-06-02T19:29:05.241475Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(mu_oracle, label=\"$\\mu(x)$\")\n",
    "plt.fill(np.concatenate([torch.arange(sigma_oracle.size(0)),\n",
    "                         torch.arange(sigma_oracle.size(0), 0, -1)]),\n",
    "         np.concatenate([mu_oracle - 1.96 * sigma_oracle,\n",
    "                         torch.flip(mu_oracle + 1.96 * sigma_oracle, dims=(0,))]),\n",
    "         alpha=0.3, fc=\"b\", ec=\"None\", label=r\"95\\% CI\")\n",
    "plt.xlabel(\"Sequence ($x$)\")\n",
    "plt.ylabel(\"Fitness ($y$)\")\n",
    "plt.legend()\n",
    "plt.title(\"Protein fitness (PDB: 3GB1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:29:08.873004Z",
     "start_time": "2020-06-02T19:29:08.859942Z"
    }
   },
   "outputs": [],
   "source": [
    "mu_oracle = mu_oracle.view(num_mutation_sites, vocab_size)\n",
    "# # TODO: Impute WT sequences to have fitness=1???\n",
    "# means[torch.floor(wt_pos_idx).long(), torch.floor(wt_aa_idx).long()] = 1.\n",
    "# sns.heatmap(means, square=True, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:29:11.666990Z",
     "start_time": "2020-06-02T19:29:11.643591Z"
    }
   },
   "outputs": [],
   "source": [
    "topk = 5\n",
    "max_pos = torch.max(mu_oracle, dim=-1) # max value per position\n",
    "# Sort in descending order so numbers can be displayed properly\n",
    "topk_pos_idx = torch.sort(max_pos.values, descending=True).indices[:topk]\n",
    "topk_aa_idx = max_pos.indices[topk_pos_idx]\n",
    "topk_vals = max_pos.values[topk_pos_idx]\n",
    "\n",
    "# Create (numbered) annotation labels\n",
    "topk_labels = torch.zeros_like(mu_oracle).long()\n",
    "topk_labels[topk_pos_idx, topk_aa_idx] = torch.arange(1, topk_pos_idx.size(0)+1)\n",
    "\n",
    "# Create (wildtype) annotation labels\n",
    "template = list(\"MTYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTE\")\n",
    "Xaa_wt = torch.Tensor(preprocessor.get_input_feats(template))\n",
    "wt_pos_idx = torch.arange(len(positions)).float()\n",
    "wt_aa_idx = Xaa_wt[positions]\n",
    "# Workaround: adjust positions so that scatter points are centered on heatmap \n",
    "wt_pos_idx += 0.5\n",
    "wt_aa_idx += 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:29:14.529963Z",
     "start_time": "2020-06-02T19:29:14.477537Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: we add 1 to index since python index starts at 0\n",
    "df = pd.DataFrame(mu_oracle.numpy(), index=positions+1, columns=seqs).rename_axis(\n",
    "    \"VP Position\", axis=\"index\").rename_axis(\"Amino Acid\", axis=\"columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:29:20.077234Z",
     "start_time": "2020-06-02T19:29:17.515956Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "# Plot topk labels on top of other mutations so that numbers can be displayed\n",
    "ax = sns.heatmap(df, cmap=\"coolwarm\", square=True)\n",
    "ax = sns.heatmap(df, cmap=\"coolwarm\", square=True, annot=topk_labels, fmt=\"d\",\n",
    "            mask=(topk_labels==0).numpy(), cbar=False)\n",
    "# Highlight topk sequences (w/ bbox)\n",
    "for pos_idx, aa_idx in zip(topk_pos_idx.numpy(), topk_aa_idx.numpy()):\n",
    "    ax.add_patch(plt.Rectangle(xy=(aa_idx, pos_idx), width=1, height=1,\n",
    "                               fill=False, edgecolor='black', lw=1.0))\n",
    "# Denote WT amino acid (w/ points)\n",
    "plt.scatter(wt_aa_idx, wt_pos_idx, marker='o', color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the positions 39,40,53 are the most informative since most amino acid substitutions result in a higher fitness score than the rest. It is, however, important to note that this only takes into account 1 mutation position. In fact, as the number of mutation sites increase, the effects of multi-site mutations no longer become additive, but rather get closer and closer to the WT fitness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory (sequence space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that oracle optimized using the CbAS algorithm is better than an oracle that leads you astray into unknown regions, say one that picks random sequences from the search space.\n",
    "\n",
    "TODO: We have yet to plot the sequences iteratively. Rather, we just concatenate all the samples together from all the iterations. The benefit of (iteratively) plotting the sequences is that it shows how the algorithm picks the next set of sequences to sample, which can be beneficial to understand the dynamics (in a Monte Carlo sense)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:29:26.131928Z",
     "start_time": "2020-06-02T19:29:26.109177Z"
    }
   },
   "outputs": [],
   "source": [
    "from examples.gb1 import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:29:30.024467Z",
     "start_time": "2020-06-02T19:29:29.890185Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: How do we not save the data since values can be quickly computed...\n",
    "dataset = data.load_variants(\"lstm\", \"Fitness\", rootdir=\"../../data/3gb1/processed/\",\n",
    "                             num_data=10000, vocab=\"aa20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:29:34.027362Z",
     "start_time": "2020-06-02T19:29:34.006182Z"
    }
   },
   "outputs": [],
   "source": [
    "all_oracles.keys(), all_gps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:29:47.727935Z",
     "start_time": "2020-06-02T19:29:39.897354Z"
    }
   },
   "outputs": [],
   "source": [
    "Vaa = dataset[:][\"arr_0\"].long()\n",
    "# Convert to onehot\n",
    "V = torch.zeros(*Vaa.size(), vocab_size)\n",
    "V.scatter_(2, torch.unsqueeze(Vaa, 2), 1)\n",
    "\n",
    "# Evaluate on (g-mean) oracle to get predictions\n",
    "gmean_oracle = all_oracles[\"g-mean\"]\n",
    "gmean_oracle.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = gmean_oracle(V)\n",
    "    mu_oracle, sigma_oracle = y_pred[:,0], torch.sqrt(y_pred[:,1])\n",
    "        \n",
    "# Evaluate on (g-mean) gp to get GT predictions\n",
    "gmean_gp = all_gps[\"g-mean\"]\n",
    "mu_gp, sigma_gp = gmean_gp.predict(Vaa, return_std=True)\n",
    "mu_gp, sigma_gp = mu_gp.squeeze(), sigma_gp.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:29:51.226661Z",
     "start_time": "2020-06-02T19:29:50.367673Z"
    }
   },
   "outputs": [],
   "source": [
    "iters = 50\n",
    "random_results = {\n",
    "    \"y_oracle\": mu_oracle.view(iters, -1),\n",
    "    \"y_gt\": mu_gp.view(iters, -1),\n",
    "    \"seq\": np.array([\"\".join(tokenizer.decode(seq)) for seq in Vaa.numpy()]).reshape(50, -1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:29:53.935400Z",
     "start_time": "2020-06-02T19:29:53.898678Z"
    }
   },
   "outputs": [],
   "source": [
    "# What are the topk sequences and their predicted oracle/GT values?\n",
    "random_y_oracle = np.array(random_results[\"y_oracle\"]).flatten()\n",
    "topk_random_idx = np.argsort(random_y_oracle)[-10:]\n",
    "\n",
    "seq_topk_random = np.array(random_results[\"seq\"]).flatten()[topk_random_idx]\n",
    "yt_topk_random = random_y_oracle[topk_random_idx]\n",
    "yt_gt_topk_random = np.array(random_results[\"y_gt\"]).flatten()[topk_random_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:29:56.611530Z",
     "start_time": "2020-06-02T19:29:56.597385Z"
    }
   },
   "outputs": [],
   "source": [
    "yt_topk_random, yt_gt_topk_random, seq_topk_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:30:01.201848Z",
     "start_time": "2020-06-02T19:29:59.838462Z"
    }
   },
   "outputs": [],
   "source": [
    "quantile = 0.80\n",
    "sort = True\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "combined_results = {\"cbas\": cbas_results, \"random\": random_results}\n",
    "colors = cm.get_cmap(\"coolwarm\")(np.linspace(0, 1, len(combined_results)))\n",
    "for label, values, c in zip(combined_results.keys(), combined_results.values(), colors):\n",
    "    # Compute 80th percentile range for y_oracle\n",
    "    oracle_samples = np.ma.masked_invalid(values[\"y_oracle\"])\n",
    "    per = np.percentile(oracle_samples, quantile*100, axis=-1)\n",
    "    per = np.reshape(per, newshape=(oracle_samples.shape[0], 1))\n",
    "    oracle_idxs = np.where(oracle_samples > per)\n",
    "\n",
    "    # Average values for samples that are greater than quantile\n",
    "    oracle_vals = np.zeros_like(oracle_samples)\n",
    "    oracle_vals[oracle_idxs] = oracle_samples[oracle_idxs]\n",
    "    oracle_vals = oracle_vals.sum(1) / (oracle_vals != 0).sum(1)\n",
    "\n",
    "    gt_samples = np.ma.masked_invalid(values[\"y_gt\"])\n",
    "    gt_vals = np.zeros_like(gt_samples)\n",
    "    gt_vals[oracle_idxs] = gt_samples[oracle_idxs]\n",
    "    gt_vals = gt_vals.sum(1) / (gt_vals != 0).sum(1)\n",
    "\n",
    "    # Sort oracle values (ascending) when plotting\n",
    "    if sort:\n",
    "        oracle_idxs = np.argsort(oracle_vals)\n",
    "        oracle_vals = oracle_vals[oracle_idxs]\n",
    "        gt_vals = gt_vals[oracle_idxs]\n",
    "\n",
    "    # Plot values\n",
    "    plt.plot(np.arange(1, len(oracle_vals)+1), oracle_vals, color=c, linestyle=\"dashed\")\n",
    "    plt.plot(np.arange(1, len(gt_vals)+1), gt_vals, color=c, label=label)\n",
    "plt.xlabel(\"Timestep ($t$)\")\n",
    "plt.ylabel(f\"${quantile * 100:.0f}\\%$ Fitness ($y$)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:30:33.194448Z",
     "start_time": "2020-06-02T19:30:33.051673Z"
    }
   },
   "outputs": [],
   "source": [
    "pos = [38, 39, 40, 53]\n",
    "char_seqs = [\"\".join(tokenizer.decode(seq)) for seq in Vaa[:, pos].numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:30:36.572039Z",
     "start_time": "2020-06-02T19:30:36.016280Z"
    }
   },
   "outputs": [],
   "source": [
    "combs = generator.gen(n=2)\n",
    "fitness_df = pd.DataFrame(0.0, index=combs, columns=combs)\n",
    "# Place fitness values into df\n",
    "for variant, fitness in zip(char_seqs, mu_oracle):\n",
    "    x, y = variant[:2], variant[2:]\n",
    "    fitness_df.at[x,y] = fitness\n",
    "fitness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:30:43.807691Z",
     "start_time": "2020-06-02T19:30:39.171054Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "sns.heatmap(fitness_df, cmap='Blues', square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilbert curve projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:30:47.000173Z",
     "start_time": "2020-06-02T19:30:46.948465Z"
    }
   },
   "outputs": [],
   "source": [
    "def hilbert(order):\n",
    "    U = (-1, 0)\n",
    "    R = (0, 1)\n",
    "    D = (1, 0)\n",
    "    L = (0, -1)\n",
    "\n",
    "    URDR = (U, R, D, R)\n",
    "    RULU = (R, U, L, U)\n",
    "    URDD = (U, R, D, D)\n",
    "    LDRR = (L, D, R, R)\n",
    "    RULL = (R, U, L, L)\n",
    "    DLUU = (D, L, U, U)\n",
    "    LDRD = (L, D, R, D)\n",
    "    DLUL = (D, L, U, L)\n",
    "\n",
    "    inception = {\n",
    "        URDR: (RULU, URDR, URDD, LDRR),\n",
    "        RULU: (URDR, RULU, RULL, DLUU),\n",
    "        URDD: (RULU, URDR, URDD, LDRD),\n",
    "        LDRR: (DLUL, LDRD, LDRR, URDR),\n",
    "        RULL: (URDR, RULU, RULL, DLUL),\n",
    "        DLUU: (LDRD, DLUL, DLUU, RULU),\n",
    "        LDRD: (DLUL, LDRD, LDRR, URDD),\n",
    "        DLUL: (LDRD, DLUL, DLUU, RULL)\n",
    "    }\n",
    "\n",
    "    pos = [(2 ** order) - 1, 0, 0]  # y, x, linear\n",
    "\n",
    "    def walk(steps, level):\n",
    "        if level > 1:\n",
    "            for substeps in inception[steps]:\n",
    "                for subpos in walk(substeps, level - 1):\n",
    "                    yield subpos\n",
    "        else:\n",
    "            for step in steps:\n",
    "                yield pos\n",
    "                pos[0] += step[0]  # y\n",
    "                pos[1] += step[1]  # x\n",
    "                pos[2] += 1  # linear\n",
    "\n",
    "    return walk(URDR, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:30:48.488343Z",
     "start_time": "2020-06-02T19:30:47.010950Z"
    }
   },
   "outputs": [],
   "source": [
    "order = 9\n",
    "ntotal = (2**order)**2\n",
    "loc = np.zeros((ntotal, 2), dtype=np.int)\n",
    "for point in hilbert(order):\n",
    "    loc[point[-1]] = point[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:30:54.224905Z",
     "start_time": "2020-06-02T19:30:53.702588Z"
    }
   },
   "outputs": [],
   "source": [
    "combs_4char = generator.gen(n=4)\n",
    "seq_val_map = dict(zip(char_seqs, mu_oracle))\n",
    "mask = np.array([seq_val_map.get(seq, 0) for seq in combs_4char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:31:40.377799Z",
     "start_time": "2020-06-02T19:31:39.356381Z"
    }
   },
   "outputs": [],
   "source": [
    "vals = np.zeros(ntotal)\n",
    "vals[:len(mask)] = mask\n",
    "\n",
    "# Place values at specified locations\n",
    "out = np.zeros((2**order, 2**order))\n",
    "for (x,y), v in zip(loc, vals):\n",
    "    out[x,y] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:32:15.175834Z",
     "start_time": "2020-06-02T19:32:09.388944Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "sns.heatmap(out, cmap=\"Blues\", square=True, mask=(out == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CbAS-generated sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:01:36.813499Z",
     "start_time": "2020-06-02T20:01:36.702293Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../../dumps/3gb1/cbas/oracle=g-mean__gp=g-mean.json\", \"rb\") as json_file:\n",
    "    results = json.load(json_file)\n",
    "gmean_seqs_cbas = np.array(results[\"seq\"])\n",
    "y_cbas = np.array(results[\"y_oracle\"])\n",
    "y_gt = np.array(results[\"y_gt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:01:54.818712Z",
     "start_time": "2020-06-02T20:01:54.603500Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# Flatten; we don't care about the iterative samples plot (for now)\n",
    "seqs_cbas = gmean_seqs_cbas.flatten()\n",
    "y_cbas = y_cbas.flatten()\n",
    "# Remove sequences that are invalid (aka None)\n",
    "valid_mask = seqs_cbas != None\n",
    "valid_seqs = seqs_cbas[valid_mask]\n",
    "valid_y = y_cbas[valid_mask]\n",
    "# Remove sequences that don't follow WT template (except at mutated positions)\n",
    "template = list(\"MTYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTE\")\n",
    "aa20_regex = \"|\".join(list(tokenizer.vocab.keys()))\n",
    "pos = [38, 39, 40, 53]\n",
    "for idx in pos:\n",
    "    template[idx] = f\"[{aa20_regex}]\"\n",
    "regex_pattern = \"\".join(template)\n",
    "\n",
    "seq_mask = np.array([True if re.findall(regex_pattern, seq, re.IGNORECASE) != [] else False\n",
    "                     for seq in valid_seqs])\n",
    "\n",
    "valid_seqs = valid_seqs[seq_mask]\n",
    "valid_y = valid_y[seq_mask]\n",
    "# Keep only first instance of unique seqs\n",
    "_, unique_idx = np.unique(valid_seqs, return_index=True)\n",
    "unique_seqs = valid_seqs[unique_idx]\n",
    "unique_y = valid_y[unique_idx]\n",
    "# Split each seq into individual char\n",
    "unique_seqs = np.array([list(seq) for seq in unique_seqs])\n",
    "\n",
    "unique_seqs.shape, unique_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:02:14.623855Z",
     "start_time": "2020-06-02T20:02:14.271071Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_char_seqs = [\"\".join(seq) for seq in unique_seqs[:, pos]]\n",
    "combs = generator.gen(n=2)\n",
    "fitness_df = pd.DataFrame(0.0, index=combs, columns=combs)\n",
    "# Place fitness values into df\n",
    "for variant, fitness in zip(unique_char_seqs, unique_y):\n",
    "    x, y = variant[:2], variant[2:]\n",
    "    fitness_df.at[x,y] = fitness\n",
    "fitness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:02:39.900430Z",
     "start_time": "2020-06-02T20:02:33.747955Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "sns.heatmap(fitness_df, cmap='Purples', square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the CbAS procedure focuses on looking at sequences that are good (based off oracle). This can be observed by the fact that certain lines are more densely sampled and \"observed\". Compared to random sequences, notice that the values predicted are (on average) better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilbert curve projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:02:58.626912Z",
     "start_time": "2020-06-02T20:02:57.268217Z"
    }
   },
   "outputs": [],
   "source": [
    "order = 9\n",
    "ntotal = (2**order)**2\n",
    "loc = np.zeros((ntotal, 2), dtype=np.int)\n",
    "for point in hilbert(order):\n",
    "    loc[point[-1]] = point[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:03:18.614378Z",
     "start_time": "2020-06-02T20:03:18.026237Z"
    }
   },
   "outputs": [],
   "source": [
    "combs_4char = generator.gen(n=4)\n",
    "seq_val_map = dict(zip([\"\".join(seq) for seq in unique_char_seqs], unique_y))\n",
    "\n",
    "mask = np.array([seq_val_map.get(seq, 0) for seq in combs_4char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:03:23.758944Z",
     "start_time": "2020-06-02T20:03:22.787710Z"
    }
   },
   "outputs": [],
   "source": [
    "vals = np.zeros(ntotal)\n",
    "vals[:len(mask)] = mask\n",
    "\n",
    "# Place values at specified locations\n",
    "out = np.zeros((2**order, 2**order))\n",
    "for (x,y), v in zip(loc, vals):\n",
    "    out[x,y] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:03:35.774407Z",
     "start_time": "2020-06-02T20:03:27.406435Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "sns.heatmap(out, cmap=\"Purples\", square=True, mask=(out == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random vs designed\n",
    "\n",
    "Subset the plots by number of mutations from the WT (i.e. hamming distance). What we hope to observe, by using this approach, is that designing protein sequences (with varying amounts of point mutations), does, in fact, result in better fitness than just sampling random mutations from the search space.\n",
    "\n",
    "NOTE: In this case, it might not be better than random because our oracle is bad at determining the fitness score between sequences. From the oracle's viewpoint, most sequences have the same fitness score (with some minor variance). Maybe the proxy (GT) is a better because it shows that the CbAS is not going into \"bad\" sequence search space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:03:40.913209Z",
     "start_time": "2020-06-02T20:03:39.868756Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax = sns.kdeplot(mu_oracle.numpy(), shade=True, label=\"Random\", color=\"grey\")\n",
    "ax = sns.kdeplot(unique_y, shade=True, label=\"CbAS\", color=\"mediumseagreen\")\n",
    "ax.set_xlabel(\"Fitness ($y$)\")\n",
    "ax.set_title(f\"All Sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:03:44.133723Z",
     "start_time": "2020-06-02T20:03:43.230318Z"
    }
   },
   "outputs": [],
   "source": [
    "template = list(\"MTYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTE\")\n",
    "cbas_hamming = (unique_seqs != template).sum(axis=-1)\n",
    "random_hamming = (np.array([tokenizer.decode(seq)\n",
    "                            for seq in Vaa.numpy()]) != template).sum(axis=-1)\n",
    "mutation_classes, random_hamming_count = np.unique(random_hamming, return_counts=True)\n",
    "mutation_classes, cbas_hamming_count = np.unique(cbas_hamming, return_counts=True)\n",
    "\n",
    "if 0 in mutation_classes:\n",
    "    mutation_classes = mutation_classes[1:]\n",
    "mutation_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:03:46.440210Z",
     "start_time": "2020-06-02T20:03:46.386675Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = [\"Random\", \"CbAS\", \"Hamming\"]\n",
    "\n",
    "df = pd.DataFrame(columns=cols)\n",
    "for t in mutation_classes:\n",
    "    random_hamming_vals = mu_oracle[np.where(random_hamming == t)]\n",
    "    cbas_hamming_vals = unique_y[np.where(cbas_hamming == t)]\n",
    "    \n",
    "    max_num_vals = max(random_hamming_vals.shape[0], cbas_hamming_vals.shape[0])\n",
    "    random_hamming_class = torch.Tensor([np.nan] * max_num_vals)\n",
    "    random_hamming_class[:len(random_hamming_vals)] = random_hamming_vals\n",
    "    cbas_hamming_class = np.array([np.nan] * max_num_vals)\n",
    "    cbas_hamming_class[:len(cbas_hamming_vals)] = cbas_hamming_vals\n",
    "    hamming_class = np.array([t] * max_num_vals)\n",
    "    \n",
    "    class_dict = dict(zip(cols, [random_hamming_class,\n",
    "                                 cbas_hamming_class,\n",
    "                                 hamming_class]))\n",
    "    df = df.append(pd.DataFrame(class_dict), ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:03:49.486443Z",
     "start_time": "2020-06-02T20:03:46.451323Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute WT fitness\n",
    "wt_aa = torch.LongTensor(tokenizer.encode(template)).unsqueeze(0)\n",
    "wt = torch.zeros(*wt_aa.size(), vocab_size)\n",
    "wt.scatter_(2, torch.unsqueeze(wt_aa, 2), 1)\n",
    "oracle.eval()\n",
    "with torch.no_grad():\n",
    "    mu_wt = gmean_oracle(wt)[:, 0].squeeze().numpy()\n",
    "\n",
    "# Global params for figure\n",
    "y_min = min(df[\"Random\"].min(), df[\"CbAS\"].min())\n",
    "y_max = max(df[\"Random\"].max(), df[\"CbAS\"].max()) + 0.1\n",
    "wt_color = \"black\"\n",
    "cbas_color = \"mediumseagreen\"\n",
    "random_color = \"grey\"\n",
    "# Compute (during iteration)\n",
    "frac_above_wt = {\"random\": [], \"cbas\": []}\n",
    "\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "gs = gridspec.GridSpec(nrows=4, ncols=4, figure=fig)\n",
    "for idx, t in enumerate(mutation_classes):\n",
    "    # Create axes\n",
    "    ax = fig.add_subplot(gs[1:, idx])\n",
    "    \n",
    "    # Plot distribution\n",
    "    class_df = df.loc[df[\"Hamming\"] == t]\n",
    "    sns.kdeplot(class_df[\"Random\"], legend=False, vertical=True, shade=True,\n",
    "                color=random_color, ax=ax)\n",
    "    sns.kdeplot(class_df[\"CbAS\"], legend=False, vertical=True, shade=True,\n",
    "                color=cbas_color, ax=ax)\n",
    "    \n",
    "    # Set x,y-axis limits\n",
    "    ax.set_xlim(0, 11)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    # Make background transparent\n",
    "    ax.patch.set_alpha(0)\n",
    "    # Remove axis ticks, and labels\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel(\"Fitness ($y$)\", fontsize=26)\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "    # Remove borders (aka despine)\n",
    "    spines = [\"top\", \"right\", \"bottom\"]\n",
    "    for s in spines:\n",
    "        ax.spines[s].set_visible(False)\n",
    "        \n",
    "    # Indicate location of mu_wt \n",
    "    if idx < len(mutation_classes) - 1:\n",
    "        ax.axhline(y=mu_wt, c=wt_color, ls=\"--\")\n",
    "    else:\n",
    "        xloc = 0.6\n",
    "        yloc = (mu_wt - y_min) / (y_max - y_min)\n",
    "        ax.axhline(y=mu_wt, xmax=xloc, c=wt_color, ls=\"--\")\n",
    "        ax.text(xloc + 0.05, yloc, s=\"WT\", color=wt_color, ha=\"center\",\n",
    "                va=\"center\", transform=ax.transAxes, fontsize=26, rotation=90)\n",
    "    \n",
    "    # Label num mutations (hamming distance)\n",
    "    ax.text(-0.01, -0.02, s=f\"${t}$\", fontweight=\"bold\", ha=\"left\",\n",
    "            va=\"center\", transform=ax.transAxes, fontsize=22)\n",
    "    \n",
    "    # Label num examples in random, cbas\n",
    "    total_random = (~class_df[\"Random\"].isnull()).sum()\n",
    "    total_cbas = (~class_df[\"CbAS\"].isnull()).sum()\n",
    "    ax.text(0.02, 0.02, s=f\"${total_random}$\", color=random_color,\n",
    "            ha=\"left\", va=\"center\", transform=ax.transAxes, fontsize=24)\n",
    "    ax.text(0.02, 0.98, s=f\"${total_cbas}$\", color=cbas_color,\n",
    "            ha=\"left\", va=\"center\", transform=ax.transAxes, fontsize=24)\n",
    "    if idx == len(mutation_classes) - 1:\n",
    "        # The \\\\ in front of each word is a hack to properly align them\n",
    "        ax.text(0.10, 0.15, s=r\"\\textbf{\\\\Random\\\\mutants}\", color=random_color,\n",
    "                ha=\"left\", va=\"center\", transform=ax.transAxes, fontsize=24)\n",
    "        ax.text(0.10, 0.75, s=r\"\\textbf{\\\\Designed\\\\mutants}\", color=cbas_color,\n",
    "                ha=\"left\", va=\"center\", transform=ax.transAxes, fontsize=24)\n",
    "        ax.text(0.3, 0.94, s=r\"\\textit{Number of samples}\", color=cbas_color,\n",
    "                ha=\"center\", va=\"center\", transform=ax.transAxes, fontsize=24)\n",
    "    \n",
    "    # Compute fraction above wt fitness\n",
    "    random_frac_above_wt = (class_df[\"Random\"] > mu_wt).sum() / total_random\n",
    "    cbas_frac_above_wt = (class_df[\"CbAS\"] > mu_wt).sum() / total_cbas\n",
    "    frac_above_wt[\"random\"].append(random_frac_above_wt)\n",
    "    frac_above_wt[\"cbas\"].append(cbas_frac_above_wt)\n",
    "    \n",
    "    # Center global x-axis label\n",
    "    if idx == (len(mutation_classes) // 2) - 1:\n",
    "        ax.set_xlabel(\"Mutations\", labelpad=25, fontsize=26)\n",
    "\n",
    "# Plot fraction above wt (for both )\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "ax1.plot(frac_above_wt[\"random\"], c=random_color, linewidth=3, label=\"Random\")\n",
    "ax1.plot(frac_above_wt[\"cbas\"], c=cbas_color, linewidth=3, label=\"Designed\")\n",
    "formula = (r\"$\\frac{\\left\\vert \\{ y_{\\mathrm{oracle}} | y_{\\mathrm{oracle}} > \" +\n",
    "           r\"y_{\\mathrm{wt}}\\} \\right\\vert}{\\left\\vert \\{y_{\\mathrm{oracle}}\\} \\right\\vert}$\")\n",
    "ax1.text(0.5, 0.85, s=formula, color=\"black\", ha=\"center\",\n",
    "        va=\"center\", transform=ax1.transAxes, fontsize=30)\n",
    "# ax1.text(0.25, 0.10, s=r\"\\textbf{Random mutants}\", color=random_color,\n",
    "#         ha=\"left\", va=\"center\", transform=ax1.transAxes, fontsize=16)\n",
    "# ax1.text(0.6, 0.58, s=r\"\\textbf{Designed mutants}\", color=cbas_color,\n",
    "#         ha=\"left\", va=\"center\", transform=ax1.transAxes, fontsize=16)\n",
    "ax1.set_xlim(0, len(mutation_classes) - 1)\n",
    "ax1.set_xticks(np.arange(0, len(mutation_classes) - 1, 1))\n",
    "ax1.set_xticklabels([])\n",
    "ax1.set_ylabel(\"Fraction above WT\", fontsize=26)\n",
    "ax1.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=-0.5) # overlap graphs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by: https://sci-hub.tw/10.1126/science.aaw2900\n",
    "http://www.ipam.ucla.edu/abstract/?tid=15908&pcode=LCO2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP (Ground Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:08:14.203264Z",
     "start_time": "2020-06-02T20:03:53.930065Z"
    }
   },
   "outputs": [],
   "source": [
    "Vaa = dataset[:][\"arr_0\"].long()\n",
    "\n",
    "# Evaluate on (g-mean) GP to get GT predictions\n",
    "gmean_gp = all_gps[\"all\"]\n",
    "mu_gt, sigma_gt = gmean_gp.predict(Vaa, return_std=True)\n",
    "mu_gt, sigma_gt = mu_gt.squeeze().numpy(), sigma_gt.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:08:20.137505Z",
     "start_time": "2020-06-02T20:08:19.683794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Flatten; we don't care about the iterative samples plot (for now)\n",
    "seqs_cbas = gmean_seqs_cbas.flatten()\n",
    "y_gt = y_gt.flatten()\n",
    "# Remove sequences that are invalid (aka None)\n",
    "valid_mask = seqs_cbas != None\n",
    "valid_seqs = seqs_cbas[valid_mask]\n",
    "valid_y_gt = y_gt[valid_mask]\n",
    "# Remove sequences that don't follow WT template (except at mutated positions)\n",
    "template = list(\"MTYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTE\")\n",
    "aa20_regex = \"|\".join(list(tokenizer.vocab.keys()))\n",
    "pos = [38, 39, 40, 53]\n",
    "for idx in pos:\n",
    "    template[idx] = f\"[{aa20_regex}]\"\n",
    "regex_pattern = \"\".join(template)\n",
    "\n",
    "seq_mask = np.array([True if re.findall(regex_pattern, seq, re.IGNORECASE) != [] else False\n",
    "                     for seq in valid_seqs])\n",
    "\n",
    "valid_seqs = valid_seqs[seq_mask]\n",
    "valid_y_gt = valid_y_gt[seq_mask]\n",
    "# Keep only first instance of unique seqs\n",
    "_, unique_idx = np.unique(valid_seqs, return_index=True)\n",
    "unique_seqs = valid_seqs[unique_idx]\n",
    "unique_y_gt = valid_y_gt[unique_idx]\n",
    "# Split each seq into individual char\n",
    "unique_seqs = np.array([list(seq) for seq in unique_seqs])\n",
    "\n",
    "unique_seqs.shape, unique_y_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:08:25.899023Z",
     "start_time": "2020-06-02T20:08:24.585603Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax = sns.kdeplot(mu_gt, shade=True, label=\"Random (GT)\", color=\"grey\")\n",
    "ax = sns.kdeplot(unique_y_gt, shade=True, label=\"CbAS (GT)\", color=\"mediumseagreen\")\n",
    "ax.set_xlabel(\"Fitness ($y$)\")\n",
    "ax.set_title(f\"All Sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:08:30.483071Z",
     "start_time": "2020-06-02T20:08:29.225817Z"
    }
   },
   "outputs": [],
   "source": [
    "template = list(\"MTYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTE\")\n",
    "cbas_hamming = (unique_seqs != template).sum(axis=-1)\n",
    "random_hamming = (np.array([tokenizer.decode(seq)\n",
    "                            for seq in Vaa.numpy()]) != template).sum(axis=-1)\n",
    "mutation_classes, random_hamming_count = np.unique(random_hamming, return_counts=True)\n",
    "mutation_classes, cbas_hamming_count = np.unique(cbas_hamming, return_counts=True)\n",
    "\n",
    "if 0 in mutation_classes:\n",
    "    mutation_classes = mutation_classes[1:]\n",
    "mutation_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:08:33.127729Z",
     "start_time": "2020-06-02T20:08:33.066727Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = [\"Random\", \"CbAS\", \"Hamming\"]\n",
    "\n",
    "df = pd.DataFrame(columns=cols)\n",
    "for t in mutation_classes:\n",
    "    random_hamming_vals = mu_gt[np.where(random_hamming == t)]\n",
    "    cbas_hamming_vals = unique_y_gt[np.where(cbas_hamming == t)]\n",
    "    \n",
    "    max_num_vals = max(random_hamming_vals.shape[0], cbas_hamming_vals.shape[0])\n",
    "    random_hamming_class = np.array([np.nan] * max_num_vals)\n",
    "    random_hamming_class[:len(random_hamming_vals)] = random_hamming_vals\n",
    "    cbas_hamming_class = np.array([np.nan] * max_num_vals)\n",
    "    cbas_hamming_class[:len(cbas_hamming_vals)] = cbas_hamming_vals\n",
    "    hamming_class = np.array([t] * max_num_vals)\n",
    "    \n",
    "    class_dict = dict(zip(cols, [random_hamming_class,\n",
    "                                cbas_hamming_class,\n",
    "                                hamming_class]))\n",
    "    df = df.append(pd.DataFrame(class_dict), ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:08:40.993121Z",
     "start_time": "2020-06-02T20:08:36.067797Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute WT fitness\n",
    "wt_aa = torch.LongTensor(tokenizer.encode(template)).unsqueeze(0)\n",
    "mu_wt = gmean_gp.predict(wt_aa, return_std=False).item()\n",
    "\n",
    "# Global params for figure\n",
    "y_min = min(df[\"Random\"].min(), df[\"CbAS\"].min())\n",
    "y_max = max(df[\"Random\"].max(), df[\"CbAS\"].max()) + 0.1\n",
    "wt_color = \"black\"\n",
    "cbas_color = \"mediumseagreen\" # \"mediumslateblue\"\n",
    "random_color = \"grey\"\n",
    "# Compute (during iteration)\n",
    "frac_above_wt = {\"random\": [], \"cbas\": []}\n",
    "\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "gs = gridspec.GridSpec(nrows=4, ncols=4, figure=fig)\n",
    "for idx, t in enumerate(mutation_classes):\n",
    "    # Create axes\n",
    "    ax = fig.add_subplot(gs[1:, idx])\n",
    "    \n",
    "    # Plot distribution\n",
    "    class_df = df.loc[df[\"Hamming\"] == t]\n",
    "    sns.kdeplot(class_df[\"Random\"], legend=False, vertical=True, shade=True,\n",
    "                color=random_color, ax=ax)\n",
    "    sns.kdeplot(class_df[\"CbAS\"], legend=False, vertical=True, shade=True,\n",
    "                color=cbas_color, ax=ax)\n",
    "    \n",
    "    # Set x,y-axis limits\n",
    "    ax.set_xlim(0, 2.5)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    # Make background transparent\n",
    "    ax.patch.set_alpha(0)\n",
    "    # Remove axis ticks, and labels\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel(\"Fitness ($y$)\", fontsize=20)\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "    # Remove borders (aka despine)\n",
    "    spines = [\"top\", \"right\", \"bottom\"]\n",
    "    for s in spines:\n",
    "        ax.spines[s].set_visible(False)\n",
    "        \n",
    "    # Indicate location of mu_wt \n",
    "    if idx < len(mutation_classes) - 1:\n",
    "        ax.axhline(y=mu_wt, c=wt_color, ls=\"--\")\n",
    "    else:\n",
    "        xloc = 0.6\n",
    "        yloc = (mu_wt - y_min) / (y_max - y_min)\n",
    "        ax.axhline(y=mu_wt, xmax=xloc, c=wt_color, ls=\"--\")\n",
    "        ax.text(xloc + 0.05, yloc, s=\"WT\", color=wt_color, ha=\"center\",\n",
    "                va=\"center\", transform=ax.transAxes, fontsize=16, rotation=90)\n",
    "    \n",
    "    # Label num mutations (hamming distance)\n",
    "    ax.text(-0.01, -0.02, s=f\"${t}$\", fontweight=\"bold\", ha=\"left\",\n",
    "            va=\"center\", transform=ax.transAxes, fontsize=18)\n",
    "    \n",
    "    # Label num examples in random, cbas\n",
    "    total_random = (~class_df[\"Random\"].isnull()).sum()\n",
    "    total_cbas = (~class_df[\"CbAS\"].isnull()).sum()\n",
    "    ax.text(0.02, 0.02, s=f\"${total_random}$\", color=random_color,\n",
    "            ha=\"left\", va=\"center\", transform=ax.transAxes, fontsize=16)\n",
    "    ax.text(0.02, 0.98, s=f\"${total_cbas}$\", color=cbas_color,\n",
    "            ha=\"left\", va=\"center\", transform=ax.transAxes, fontsize=16)\n",
    "    if idx == len(mutation_classes) - 1:\n",
    "        ax.text(0.10, 0.15, s=r\"\\textbf{Random mutants}\", color=random_color,\n",
    "                ha=\"left\", va=\"center\", transform=ax.transAxes, fontsize=16)\n",
    "        ax.text(0.10, 0.75, s=r\"\\textbf{Designed mutants}\", color=cbas_color,\n",
    "                ha=\"left\", va=\"center\", transform=ax.transAxes, fontsize=16)\n",
    "        ax.text(0.3, 0.96, s=r\"\\textit{Number of samples}\", color=cbas_color,\n",
    "                ha=\"center\", va=\"center\", transform=ax.transAxes, fontsize=16)\n",
    "    \n",
    "    # Compute fraction above wt fitness\n",
    "    random_frac_above_wt = (class_df[\"Random\"] > mu_wt).sum() / total_random\n",
    "    cbas_frac_above_wt = (class_df[\"CbAS\"] > mu_wt).sum() / total_cbas\n",
    "    frac_above_wt[\"random\"].append(random_frac_above_wt)\n",
    "    frac_above_wt[\"cbas\"].append(cbas_frac_above_wt)\n",
    "    \n",
    "    # Center global x-axis label\n",
    "    if idx == (len(mutation_classes) // 2) - 1:\n",
    "        ax.set_xlabel(\"Mutations\", labelpad=25, fontsize=20)\n",
    "\n",
    "# Plot fraction above wt (for both )\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "ax1.plot(frac_above_wt[\"random\"], c=random_color, linewidth=3, label=\"Random\")\n",
    "ax1.plot(frac_above_wt[\"cbas\"], c=cbas_color, linewidth=3, label=\"Designed\")\n",
    "# ax1.text(0.25, 0.10, s=r\"\\textbf{Random mutants}\", color=random_color,\n",
    "#         ha=\"left\", va=\"center\", transform=ax1.transAxes, fontsize=16)\n",
    "# ax1.text(0.6, 0.58, s=r\"\\textbf{Designed mutants}\", color=cbas_color,\n",
    "#         ha=\"left\", va=\"center\", transform=ax1.transAxes, fontsize=16)\n",
    "ax1.set_xlim(0, len(mutation_classes) - 1)\n",
    "ax1.set_xticks(np.arange(0, len(mutation_classes) - 1, 1))\n",
    "ax1.set_xticklabels([])\n",
    "ax1.set_ylabel(\"Fraction above WT fitness\", fontsize=20)\n",
    "ax1.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=-0.5) # overlap graphs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:08:49.403148Z",
     "start_time": "2020-06-02T20:08:45.006888Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n",
    "\n",
    "for ax, t in zip(axarr.flatten(), mutation_classes):\n",
    "    cbas_hamming_mean_gt = unique_y_gt[np.where(cbas_hamming == t)]\n",
    "    random_hamming_mean_gt = mu_gt[np.where(random_hamming == t)]\n",
    "    sns.kdeplot(random_hamming_mean_gt, shade=True, label=\"Random (GT)\", color=\"grey\", ax=ax)\n",
    "    sns.kdeplot(cbas_hamming_mean_gt, shade=True, label=\"CbAS (GT)\", color=\"mediumseagreen\", ax=ax)\n",
    "    ax.set_title(f\"Mutations (n=${t}$)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
